---
title: "Book Recommendations via Neural Collaborative Filtering"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```


```{r echo=FALSE, cache = TRUE}
path <- "/data/"
interactions <- read_csv(str_c(path, "goodreads_interactions.csv"), col_names = TRUE)
book_id_map <- read_csv(str_c(path, "book_id_map.csv"), col_names = TRUE) # Need for linking to book info dataset
system("tar -xzf ../Data/goodreads_books_christian.tar.gz")
load("goodreads_books_christian.RData")
system("rm goodreads_books_christian.RData")

```


Over the past 1.5 years I have enjoyed being part of a Christian fiction book club. 
Sadly our book club is ending as our beloved leader is moving to Ohio. I plan to continue reading books, but 
I also have an interest in recommender systems. So, I thought it would be fun to recommend new books for my book club. 

## The technical details

In this post, I'll be using [Neural Collaborative Filtering](https://dl.acm.org/doi/10.1145/3038912.3052569). I created a version of the model in R Keras, which is available on [GitHub](https://github.com/AmandaRP/NeuralCollaborativeFiltering).

The model is designed for binary implicit feedback (positive and negative preferences that we glean from the user's history).


# The Data

To train the model I used a data set from [GoodReads.com](https://en.wikipedia.org/wiki/Goodreads), a social book cataloging system. The data is available on the [USCD Book Graph webiste](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph). Specifically, I'm using three files provided on this website, each providing the following fields:

* **interactions**: Contains fields `user_id`, `book_id`, `is_read`, `rating`, `is_reviewed`
* **book_info**: Information about the books containing `r ncol(book_info)` fields including title, isbn, author(s), and list of shelves. 
* **book_id_map**: books ids used to join the other two data sets together

Note that this data set is a bit outdated (only 6 books from 2019, none from 2020). It would be nice to obtain an updated dat set. 


## Filtering by genre

```{r echo = FLASE}
n_interactions_before_filtering <- nrow(interactions)
```


The `shelves` field is a list of virtual shelves that users have added a book to. The name is chosen by the user and can be anything such as "to read", "book club", or the name of the genre. By looking at the list of shelves, we can make some inferences about the book's genre. To limit books for my book club to Christian fiction, I included books that were on at least one shelf with "christian" in the name and at least one shelf with "fiction" in the name (ignoring "nonfiction" and "non-fiction"). This isn't a perfect method for determining genre, but it significantly reduced the data size (228,648,342 interactions reduced to 5,628,062, a 98% reduction).   


## Infering Likes and Dislikes

For the NCF model, we need a list of books that each user liked and a list that they did not like, which we can deduce from the `interactions` data. Five randomly sampled rows of this data is shown below

```{r}
slice_sample(interactions, n=5)
```

We'll assume that a reader likes the book if:

1. They read the book AND (either gave it 4 or 5 stars OR didn't provide rating). That is:
`(is_read == 1 & (0 == rating | rating >= 4))`, OR
2. The book is on their shelf and not read yet. That is, `(is_read == 0)`.

This is an optimistic viewpoint because we may include books that the reader later reads and decides that they don't like. 

We'll also assume that a reader did not like a book if they read it and gave it a rating of 1 or 2. Books with a score of 3 were not used for model training. 

Note that there exist cases where no rating has been provided (value of `rating` is 0), but a review exists. I think an enhancement for obtaining more positive and negative examples would be to mine the sentiment of reviews for these instances.

After performing these steps, I filtered out users who did not have at least 3 books that they liked (a threshold that could be tweaked). TODO: Show histogram of num books per user.


```{r eval = FALSE}
#TODO: change this dataset to filtered dataset.
interactions %>% 
  group_by(user_id) %>%
  count() %>% arrange(desc(n))
  #ggplot(aes(n)) + 
  #geom_histogram()
```


## Data for my book club

I added our club's books to the data set. We have read dozens of books, but the following were those that we had a strong opinion about (either "good" and "bad").


"Good" books:

Books we didn't like:


Other books: 
(others not listed were rated in the middle and not useful for training)
We would have also used the following books for training, but they weren't in the dataset:
The 49th Mistic https://www.goodreads.com/book/show/36548233-the-49th-mystic (good or bad?)
My Heart Belongs in Blue Ridge

## Train/Validation/Test Split


Method (following the paper):
- Training: 4 negatives for every positive (for each user)
- Validation: 1 positive (for each user) 
- Test: 1 positive, 100 negative (for each user)


## Results

```{r}
load("recommendations_12_2.rds")

#plot:
p1 <- recommendations %>% 
  ggplot(aes(pred)) + 
  geom_histogram() + 
  ggtitle("Distribution of Predicted Book Scores for Reading Group") + 
  xlab("Predicted Score") +
  ylab("Book Count") + 
  theme_classic()
p1

```

```{r}
# The following works, but I'm thinking there must be a more elegant method.
rec_m_12_2 <- rec_m_12_2 %>% mutate(francinerivers = map_lgl(authors, ~filter(., author_id == 6492) %>% nrow() > 0)) 

```

```{r}
p2 <- rec_m_12_2 %>% 
  mutate(author1 = if_else(francinerivers == TRUE, "Francine Rivers", "Other")) %>%
  filter(pred >= 0.5) %>%
  ggplot(aes(pred, fill = author1)) +
  geom_histogram(alpha=0.7, position="identity") + 
  #guides(fill = guide_legend(reverse = TRUE)) +
  theme_classic() + 
  theme(legend.title = element_blank()) + 
  labs(y = element_blank(), x = element_blank())
p2
```

```{r}
library(patchwork)

p1 + inset_element(p2, left = 0.3, bottom = 0.3, right = 1, top = .9)
```


  

# Thoughts on why I may not be getting good recommendations:
1. My group's training data may not be large enough. I may need a model that is better for the cold start, i.e. a model that takes side features such as author and book description.
2. Due to hardware limitations and slow model training, I didn't perform hyper-parameter tuning. Rather I used the defaults that were used in the NCF paper. (I am using a TODO AWS instance, training is slow (~7 hours per epoch).) A grid search on the hyperparamter space. 
3. Point out criticisms of NCF?

